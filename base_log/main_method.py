from DrissionPage.common import Settings
import csv
import os
from typing import Union, List, Set
from DrissionPage import Chromium,ChromiumOptions
import requests
import time
import base.global_var as gv
import threading
from typing import Optional, Union,Tuple, Literal
import json
from pathlib import Path
from base.logger_service import log_to_text


Settings.set_language('zh_cn')  # è®¾ç½®ä¸ºä¸­æ–‡æ—¶ï¼Œå¡«å…¥'zh_cn'

def ger_cookies(page,url = "https://www.therealreal.com/")->dict:
    """è·å–ç½‘ç«™çš„COOKIES"""
    # page.get(url)
    cookies_dict:dict = page.cookies(all_domains=False).as_dict()
    return cookies_dict

def set_cookies(page,cookies_dict:dict,domain= ".therealreal.com"):
    """è®¾ç½®COOKIES"""
    cookies_dict["domain"] = domain
    page.set.cookies(cookies_dict)

def write_lists_to_csv(urls: Union[List[str], Set[str]], names: List[str] = None, filename: str = 'link_data.csv',model = "a") -> None:
    """
    å°†æ•°æ®å†™å…¥CSVæ–‡ä»¶ï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. åŒæ—¶å†™å…¥å•†å“åç§°å’ŒURLï¼ˆå½“nameså‚æ•°æä¾›æ—¶ï¼‰
    2. ä»…å†™å…¥URLï¼ˆå½“nameså‚æ•°ä¸ºNoneæ—¶ï¼‰
    
    å‚æ•°:
        urls (List[str] or Set[str]): URLåˆ—è¡¨æˆ–é›†åˆ
        names (List[str], optional): å•†å“åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone
        filename (str): CSVæ–‡ä»¶åï¼Œé»˜è®¤ä¸º'link_data.csv'
        model(str):  æ¨¡å¼é€‰æ‹©ï¼Œé»˜è®¤ä¸º'a'è¿½åŠ æ¨¡å¼
    """
    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if names is not None and len(names) != len(urls):
        raise ValueError("å½“æä¾›nameså‚æ•°æ—¶ï¼Œä¸¤ä¸ªåˆ—è¡¨çš„é•¿åº¦å¿…é¡»ç›¸åŒ")

    # ç¡®å®šæ˜¯å¦éœ€è¦å†™è¡¨å¤´
    write_header = not os.path.exists(filename)

    # æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥æ•°æ®
    with open(filename, model, newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.writer(csvfile)
        
        # å†™å…¥è¡¨å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if write_header:
            header = ['è¿æ¥'] if names is None else ['å•†å“åç§°', 'è¿æ¥']
            writer.writerow(header)
        
        # å†™å…¥æ•°æ®è¡Œ
        if names is not None:
            for name, url in zip(names, urls):
                writer.writerow([name, url])
        else:
            for url in urls:
                writer.writerow([url])



def read_urls_from_csv(filename='link_data.csv'):
    """
    ä»CSVæ–‡ä»¶ä¸­è¯»å–'è¿æ¥'åˆ—å¹¶è¿”å›ä¸ºé›†åˆ
    
    å‚æ•°:
        filename (str): CSVæ–‡ä»¶åï¼Œé»˜è®¤ä¸º'link_data.csv'
        
    è¿”å›:
        set: åŒ…å«æ‰€æœ‰å”¯ä¸€URLçš„é›†åˆ
    """
    urls = set()
    
    try:
        with open(filename, 'r', newline='', encoding='utf-8-sig') as csvfile:
            reader = csv.DictReader(csvfile)
            
            # æ£€æŸ¥è¡¨å¤´æ˜¯å¦å­˜åœ¨
            if 'è¿æ¥' not in reader.fieldnames:
                raise ValueError("CSVæ–‡ä»¶ä¸­ç¼ºå°‘'è¿æ¥'åˆ—")
            
            # è¯»å–URLåˆ—
            for row in reader:
                url = row['è¿æ¥'].strip()  # å»é™¤å‰åç©ºæ ¼
                if url:  # è·³è¿‡ç©ºå€¼
                    urls.add(url)
                    
    except FileNotFoundError:
        log_to_text(f"è­¦å‘Š: æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        return set()
    
    return urls



def open_url_get_shop_url(page:Chromium,url_class: str,page_num = None) -> list:
    """æ‰“å¼€æŒ‡å®šå•†å“ç±»ç›®çš„URLé“¾æ¥"""
    start_time = time.time()
    page.set.load_mode.eager
    if page_num is  None:
        url = "https://www.therealreal.com/products?keywords=chrome%20hearts%20"+url_class
    else:
        url = "https://www.therealreal.com/products?keywords=chrome%20hearts%20"+url_class+"&page="+str(page_num)
    page.get(url)
    log_to_text(f"æ‰“å¼€é“¾æ¥è€—æ—¶: {time.time()-start_time}")

def get_shop_urls(page:Chromium,card:str): # åæœŸå¯ä»¥æ”¹æˆç›´æ¥ä¼ å…¥é¡µé¢IDï¼Œé€šè¿‡é¡µé¢IDå»è·å–ï¼Œè¿™æ ·å¯ä»¥åŒæ—¶è·å–å¤šä¸ªé¡µé¢çš„é“¾æ¥
    """è·å–é¡µé¢ä¸­çš„å•†å“é“¾æ¥ï¼Œè®°å¾—å…ˆè¦æ‰“å¼€å•†å“ç±»ç›®é¡µé¢ï¼Œä¹Ÿå°±æ˜¯open_url_get_shop_urlå‡½æ•°
    :param page: Chromiumå¯¹è±¡
    :param card: all | salable è·å–å•†å“é“¾æ¥çš„æ–¹å¼ï¼Œallè¡¨ç¤ºè·å–æ‰€æœ‰å•†å“é“¾æ¥ï¼Œsalableï¼šè¡¨ç¤ºè·å–å¯å”®å–çš„å•†å“é“¾æ¥
    ps:salableå…³é”®å­—æš‚æ—¶å¼ƒç”¨ï¼Œå› ä¸ºéœ€è¦é‡å†™è·å–å•†å“é“¾æ¥çš„é€»è¾‘ï¼Œæ‰€ä»¥æš‚æ—¶å®ç°æ‰€æœ‰å•†å“çš„è·å–æ–¹å¼
    """
    # TODO ç½‘é¡µæ˜¯é™æ€çš„ï¼ŒåæœŸå¯ä»¥è€ƒè™‘ç›´æ¥ç”¨requestsè·å–
    start_time = time.time()
    if card == "all":
        shop_list_ele = page.ele("@@class=product-grid").eles('@@class=product-card__description product-card__link js-product-card-link') # è·å–æ‰€æœ‰å•†å“é“¾æ¥
        log_to_text(f"è·å–å•†å“é“¾æ¥è€—æ—¶ï¼š"{time.time() - start_time})
    elif card == "salable":
        log_to_text("åæœŸæŒ‰éœ€å®ç°")
        raise "æš‚æœªå®ç°salableæ–¹å¼çš„è·å–å•†å“é“¾æ¥ï¼ŒåæœŸå¯èƒ½ä¼šå®ç°"
        shop_list_ele = page.ele("@@class=product-grid").eles('@@class=product-card__see-similar-items js-track-click-event') # è·å–å…‹å”®å–çš„å•†å“é“¾æ¥

    # TODO:è¿™é‡Œå¯èƒ½æ¶‰åŠåˆ°å¤šä¸ªç½‘é¡µçš„å¤„ç†ï¼ŒåæœŸä¿®æ”¹ä»£ç 
    log_to_text(f"åˆ—è¡¨æœ‰{len(shop_list_ele)}ä¸ªå•†å“")
    shop_list = []
    link_list = []
    start_time = time.time()
    for shop in shop_list_ele:
        # i = shop.ele('@@class=product-card__status-label js-product-card__status-label')
        # log_to_text(f"å¾—åˆ°çš„æ•°æ®æ˜¯{shop.text()}")
        shop_list.append(shop.text)
        link_list.append(shop.link)
    log_to_text(f"è§£æå•†å“è€—æ—¶{time.time()-start_time}ç§’")
    return shop_list,link_list

    
def parse_cookie_string(cookie_str):
    """
    å°†æµè§ˆå™¨æ ¼å¼çš„Cookieå­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
    æ”¯æŒåŒ…å«domain/pathç­‰å±æ€§å’Œå¤æ‚å€¼çš„åœºæ™¯
    """
    cookies = {}
    for item in cookie_str.strip().split(';'):
        item = item.strip()
        if not item:
            continue
        
        # å¤„ç†é”®å€¼å¯¹ï¼ˆåŒ…æ‹¬domain=xxxè¿™ç§æƒ…å†µï¼‰
        if '=' in item:
            key, value = item.split('=', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªç­‰å·
            cookies[key] = value
        else:
            # å¤„ç†åªæœ‰keyæ²¡æœ‰valueçš„å±æ€§ï¼ˆå¦‚HttpOnlyï¼‰
            cookies[item] = None
    
    return cookies




def login(page,username: str, password: str) -> str:
    """ä»¥ä¸‹æ˜¯ç™»å½•æµ‹è¯•ï¼ŒåæœŸå†æ‰“åŒ…æˆå‡½æ•°"""
    start_time = time.time()
    # æˆ–è€…å¯ä»¥ç›‘å¬https://www.therealreal.com/sessionsç½‘å€çš„è¿”å›æ˜¯å¦æ˜¯200
    first_ele = "@@class=info-link underlined-link js-track-click-event@@tabindex=0@@text()=Member Sign In"#ç™»å½•
    second_ele = "@@class=info-link underlined-link js-track-click-event@@tabindex=0@@text()=Member Sign Up" #æ³¨å†Œ
    status,idex_ele = fast_find_ele(page,first_ele,second_ele)
    if status is False:
        log_to_text("ä¸¤ä¸ªå…ƒç´ éƒ½æœªæ‰¾åˆ°")
        return idex_ele # è¿™é‡Œè¿”å›çš„æ˜¯ç©ºï¼Œè¡¨ç¤ºæœªæ‰¾åˆ°
    
    if idex_ele == "first":
        # æ‰¾åˆ°ç™»å½•å…ƒç´ ï¼Œç‚¹å‡»è¿›è¡Œç™»å½•
        log_to_text("æ‰¾åˆ°ç™»å½•å…ƒç´ ï¼Œè¿›è¡Œç‚¹å‡»åˆ‡æ¢åˆ°ç™»å½•é¡µé¢")
        page.eles("t:a@@text()=Member Sign In")[1].click()
    else:
        # æ‰¾åˆ°æ³¨å†Œå…ƒç´ ï¼Œç›´æ¥è¿›è¡Œç™»å½•
        log_to_text("æ‰¾åˆ°æ³¨å†Œå…ƒç´ ï¼Œç›´æ¥è¿›è¡Œç™»å½•")

    page.listen.start('https://www.therealreal.com/sessions')
    # è¾“å…¥ç”¨æˆ·å
    page.ele("t:input@@id=user_email@@name=user[email]").input(username,clear=True)
    #  è¾“å…¥å¯†ç 
    page.ele("@@placeholder=Password@@tabindex=0").input(password,clear=True)
    page.ele("t:input@@name=user[remember_me]@@tabindex=0").click() # ç‚¹å‡»è®°ä½æˆ‘

    page.eles("t:input@@class=button button--primary button--featured button--capped-full-width form-field__submit js-track-click-event")[1].click()
    # ç­‰å¾…åŠ è½½å®Œæˆ
    # page.wait_for_load_complete()
    res = page.listen.wait()

    log_to_text(f"ç™»å½•ç”¨æ—¶ï¼š{time.time()-start_time}ç§’")


def get_page_cont(page: Chromium):
    page_cont = page.ele("t:nav@@class=plp-header-controls__pagination ").text
    # log_to_text(f"å¾—åˆ°çš„é•¿åº¦æ˜¯ï¼š{page_cont.split("\n")}")
    return page_cont.split("\n")

def get_shop_save_csv():
    co1 = ChromiumOptions().set_local_port(9226).set_user_data_path('data1')
    page = Chromium(co1).latest_tab
    page.get('https://www.therealreal.com/')

    log_status = is_login(page) 
    if log_status is False:
        username = "r173226793@gmail.com"
        password = "Richie123.."
        # ç™»å½•
        login(page,username,password)
        # è·å–ç½‘ç«™COOKIE
    cookies_dict = ger_cookies(page)
    # è®¾ç½®ç½‘ç«™COOKIE
    set_cookies(page,cookies_dict)
    # æ‰“å¼€å•†å“æœç´¢é¡µé¢
    shop_list = ["betls","bags","jewelry"]

    if Path('link_data.csv').exists():
        log_to_text("ğŸ“ æ¸…ç†æ—§æ–‡ä»¶")
        Path('link_data.csv').unlink()

    for shop_class in shop_list:
        # éå†æ¯ä¸ªé¡µé¢
        open_url_get_shop_url(page,shop_class)
        shop_list,link_list = get_shop_urls(page,"all")
        write_lists_to_csv(urls = link_list,names = shop_list)
        page_list = get_page_cont(page) # å¾—åˆ°é¡µç åˆ—è¡¨
        # å¦‚æœåˆ†é¡µå¤§äº1ï¼Œåˆ™å¾ªç¯éå†å…¶ä»–é¡µé¢ï¼Œè¿›è¡Œä¿å­˜æ•°æ®
        if len(page_list)  > 1:
            for page_num in page_list[1:]:
                open_url_get_shop_url(page,shop_class,page_num)
                shop_list,link_list = get_shop_urls(page,"all")
                write_lists_to_csv(urls = link_list,names = shop_list)
    log_to_text("ä¿å­˜å®Œæˆ")


def start_get_shop_save_csv():
    t = threading.Thread(target=get_shop_save_csv,daemon=True)
    t.start()
    
def add_shop_car(good_id:str,session_id:str,query_id:str,cookie_dict:dict,x_csrf_token):
    """é€šè¿‡å‘é€POSTè¯·æ±‚å°†å•†å“åŠ å…¥è´­ç‰©è½¦"""
    if not good_id:
        raise ValueError("good_idä¸èƒ½ä¸ºç©º")
    if not session_id:
        raise ValueError("session_idä¸èƒ½ä¸ºç©º")
    if not query_id:
        raise ValueError("query_idä¸èƒ½ä¸ºç©º")
    if not cookie_dict:
        raise ValueError("cookie_dictä¸èƒ½ä¸ºç©º")
    headers = gv.get_global_var("headers") #  è·å–è¯·æ±‚å¤´

    headers["x-csrf-token"] = x_csrf_token # æ·»åŠ x-csrf-token

    url = 'https://www.therealreal.com/cart/items'
    form_data = {
        'id': good_id,
        '_analytics_session_id': session_id,
        'return_product_id': '',
        'protection_product_id': '',
        # 'query_id': query_id
    }
  
    # å‘é€POSTè¯·æ±‚
    start = time.time()
    response = requests.post(url, data=form_data,headers=headers,cookies=cookie_dict)
    log_to_text(f"ç­‰å¾…æœåŠ¡å™¨åŠ è´­å“åº”ç”¨æ—¶ï¼š{time.time()-start}")
    if response.status_code == 200:
        log_to_text("postè¯·æ±‚æ·»åŠ æˆåŠŸ")

    elif response.status_code == 403:
        log_to_text("é‡åˆ°äººæœºéªŒè¯")
    log_to_text(response.status_code)


def open_url_add_car(page:Chromium,url:str,):
    """é€šè¿‡ä¼ å…¥URLè¿›è¡Œæ·»åŠ è´­ç‰©è½¦"""
    log_to_text("æ­£åœ¨é€šè¿‡é¡µé¢äº¤äº’æ·»åŠ è´­ç‰©è½¦")
    page.set.load_mode.none() # å¿½ç•¥åŠ è½½
    page_shop_tab = page.new_tab(url)
    start_time = time.time()
    add_car = page_shop_tab.wait.eles_loaded("@@class=button button--primary js-pdp-add-to-cart-button",any_one=True)
    log_to_text(f"ç­‰å¾…æŒ‰é’®å‡ºç°ç”¨æ—¶ï¼š{time.time()-start_time}")
    if add_car:
        log_to_text("æ‰¾åˆ°æ·»åŠ è´­ç‰©è½¦æŒ‰é’®")
        page_shop_tab.stop_loading()
        start = time.time()
        page_shop_tab.ele("@@class=button button--primary js-pdp-add-to-cart-button").click()
        log_to_text(f"URLæ–¹å¼æ·»åŠ è´­ç‰©è½¦ç‚¹å‡»æˆåŠŸï¼Œç”¨æ—¶ï¼š{time.time()-start}")
    else:
        log_to_text("æ²¡æœ‰æ‰¾åˆ°æ·»åŠ è´­ç‰©è½¦æŒ‰é’®")
    log_to_text(f"åŠ å…¥è´­ç‰©è½¦ä¸€å…±ç”¨æ—¶ï¼š{time.time()-start_time}")



def post_add_car(page: Chromium,url,tab_id)->None:
    """
    æäº¤åŠ å…¥è´­ç‰©è½¦
    """
    page_tab = page.get_tab(tab_id)
    cookie = ger_cookies(page_tab)
    set_cookies(page_tab,cookie)
    cookie = ger_cookies(page_tab)
    css_ele  = "product-card__see-similar-items js-track-click-event"


    start = time.time()
    good_id,queryID,session_id = find_url_add_car_info(page_tab,url)
    log_to_text(f"å¾—åˆ°å•†å“ä¿¡æ¯ç”¨æ—¶{time.time()-start}ç§’")
    # è·å–å½“å‰é¡µé¢çš„å“åº”å¯¹è±¡
    start = time.time()
    x_csrf_token = page_tab.ele("t:meta@@name=csrf-token").attr("content")
    log_to_text(f"è·å–tokenç”¨æ—¶{time.time()-start}")
    # log_to_text(f"x_csrf_tokençš„å€¼æ˜¯{x_csrf_token}")
    # log_to_text(f"å¾—åˆ°çš„åŠ è´­ä¿¡æ¯æ˜¯good_idï¼š{good_id}ï¼ŒqueryIDï¼š{queryID}ï¼Œsession_id:{session_id}")
    start = time.time()
    add_shop_car(good_id,queryID,session_id,cookie,x_csrf_token)
    log_to_text(f"æ·»åŠ è´­ç‰©è½¦ç”¨æ—¶ï¼š{time.time()-start}")


def is_login(page: Chromium)->None|bool:
    """åˆ¤æ–­æ˜¯å¦å·²ç»ç™»å½•
    None:åˆ¤æ–­å¤±è´¥
    True:å·²ç»ç™»å½•
    False:æœªç™»å½•
    """
    start_time = time.time()
    first_ele = "@class=js-header-first-look" #å·²ç»ç™»å½•
    second_ele = "@class=head-utility-row__sign-up js-sign-up-link underlined-link" # æœªç™»å½•
    status,idex_ele = fast_find_ele(page,first_ele,second_ele)
    log_to_text(f"åˆ¤æ–­ç™»å½•ç”¨æ—¶ï¼š{time.time()-start_time}ç§’")
    if status is False:
        log_to_text("ä¸¤ä¸ªå…ƒç´ éƒ½æœªæ‰¾åˆ°")
        return idex_ele # è¿™é‡Œè¿”å›çš„æ˜¯ç©ºï¼Œè¡¨ç¤ºæœªæ‰¾åˆ°
    
    if idex_ele == "first":
        log_to_text("å·²ç»ç™»å½•")
        return True
    else:
        log_to_text("æœªç™»å½•")
        return False





def fast_find_ele(page: Chromium,first_ele: str,second_ele: str,timeout: float = 200.0,interval: float = 0.1) -> Tuple[bool, Literal["first", "second"] | None] :
    """
    å¿«é€Ÿå¾ªç¯æ£€æµ‹ä¸¤ä¸ªå…ƒç´ ï¼Œè¿”å›æœ€å…ˆæ‰¾åˆ°çš„å…ƒç´ 
    
    Args:
        page: Chromiumé¡µé¢å¯¹è±¡
        first_ele: ç¬¬ä¸€ä¸ªå…ƒç´ è¡¨è¾¾å¼ï¼Œå¦‚ 't:a@@text()=Member Sign In'
        second_ele: ç¬¬äºŒä¸ªå…ƒç´ è¡¨è¾¾å¼ï¼Œå¦‚ 't:input@@id=user_email'
        timeout: è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤20ç§’
        interval: æ£€æŸ¥é—´éš”(ç§’)ï¼Œé»˜è®¤0.1ç§’
    
    Returns:
        str: è¿”å›æ‰¾åˆ°çš„å…ƒç´ è¡¨è¾¾å¼
        False: è¶…æ—¶æœªæ‰¾åˆ°ä»»ä½•å…ƒç´ 
    """
    start_time = time.time()
    while time.time() - start_time < timeout:
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ 
        if page.ele(first_ele, timeout=0):
            return True,"first"
        
        # æ£€æŸ¥ç¬¬äºŒä¸ªå…ƒç´ 
        if page.ele(second_ele, timeout=0):
            return True,"second"
        
        time.sleep(interval)
    
    return False,None  # è¶…æ—¶æœªæ‰¾åˆ°ä»»ä½•å…ƒç´ 

def test_account():
    """æµ‹è¯•è¿‡éªŒè¯"""
    co1 = ChromiumOptions().set_local_port(9226).set_user_data_path('data1')
    page = Chromium(co1).latest_tab
    account_eles = page.eles("t:p@@text()=CLICK AND HOLD")[0]
    # page.actions.hold(account_eles)
    print(account_eles)
    
    

def open_url_in_tab(page, url):
    """åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€URL"""
    global_url_set = gv.get_global_var('global_url_set')
    try:
        tab = page.new_tab(url)
        # if tab.status_code == 200:
        #     log_to_text(f'æˆåŠŸæ‰“å¼€ URL: {tab}')
        # elif tab.status_code == 403:
        #     log_to_text(f'é‡åˆ°äººæœºéªŒè¯ï¼Œè¯·æ‰‹åŠ¨å¤„ç†')
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é¡µé¢æ“ä½œä»£ç 
    except Exception as e:
        log_to_text(f'æ‰“å¼€ {url} å¤±è´¥: {str(e)}')
        raise e
        return


    while True:
        """å¾ªç¯åˆ·æ–°å’Œè·å–é¡µé¢å•†å“é“¾æ¥"""
        try:
            _,link_list = get_shop_urls(tab,"all") # å¾—åˆ°é¡µé¢ä¸­çš„å•†å“é“¾æ¥
            url_set = set(link_list)
            log_to_text(f'å…¨å±€å˜é‡çš„é›†åˆæ˜¯ {len(url_set)} ä¸ª')
            add_url_set = find_difference(url_set,global_url_set) # æ‰¾å‡ºæ–°å¢çš„url
            if add_url_set:
                """è·å–å•†å“è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶è¿›è¡ŒåŠ è´­,è¿™é‡Œéœ€è¦è¿›è¡Œå¤šçº¿ç¨‹å¤„ç†ï¼Œéœ€è¦åˆ›å»ºç­‰äºadd_url_seté•¿åº¦çš„çº¿ç¨‹æ± ï¼Œç„¶åè¿›è¡Œæ‰¹é‡å¤„ç†"""
                log_to_text(f'æ–°å¢å•†å“é“¾æ¥æ•°é‡ä¸º: {len(add_url_set)}ï¼Œè¿æ¥æ˜¯{add_url_set}')
                tab_id = tab.tab_id # è·å–å½“å‰æ ‡ç­¾é¡µçš„id
                add_url_list = list(add_url_set)
                start_threads_add_cart(page,add_url_list,tab_id)

                break
            else:
                log_to_text("æ²¡æœ‰åˆ·æ–°åˆ°æ–°çš„å•†å“ï¼ŒæŒ‡å®šåˆ·æ–°æ—¶é—´ï¼Œç»§ç»­ç­‰å¾…")
                time.sleep(0.5) # è¿™é‡Œæ˜¯é¡µé¢åˆ·æ–°æ—¶é—´ï¼ŒåæœŸå¯ä»¥é€šè¿‡UIç•Œé¢è¿›è¡Œé…ç½®ï¼Œæˆ–è€…å…¨å±€å˜é‡è¿›è¡Œé…ç½®
                tab.refresh()
        except Exception as e:
            log_to_text(f'è·å–å•†å“é“¾æ¥å¤±è´¥ {str(e)}')

def open_urls_concurrently(page, urls: list):
    """å¹¶å‘æ‰“å¼€å¤šä¸ªURL"""
    threads = []
    for url in urls:
        t = threading.Thread(target=open_url_in_tab, args=(page, url),daemon=True)
        t.start()
        threads.append(t)
        time.sleep(0.1)  # ç¨å¾®å»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å…åŒæ—¶åˆ›å»ºå¤ªå¤šæ ‡ç­¾é¡µ


def start_threads_add_cart(page, urls: list,tab_id:str):
    """å¯åŠ¨å¤šçº¿ç¨‹è¿›è¡ŒåŠ è´­"""
    for url in urls:
        t = threading.Thread(target=add_cart_in_two_threads, args=(page, url,tab_id),daemon=True)
        t.start()


def add_cart_in_two_threads(page,url,tab_id):
    """å¯åŠ¨ä¸¤ç§åŠ è´­æ–¹å¼è¿›è¡Œå•†å“åŠ è´­"""


    # post_pram_url = url.replace("https://www.therealreal.com", "") # POSTæäº¤çš„æ—¶å€™ï¼ŒURLéœ€è¦è¿›è¡Œæˆªå–ï¼Œä¸åŒ…å«ä¸»é¡µçš„URL
    # post_thread = threading.Thread(target=post_add_car, args=(page,post_pram_url,tab_id),daemon=True)
    # post_thread.start()

    tab_thread = threading.Thread(target=open_url_add_car, args=(page, url),daemon=True)
    tab_thread.start()







def find_difference(set_a, set_b):
    """
    æ‰¾å‡ºåœ¨Aé›†åˆä½†ä¸åœ¨Bé›†åˆä¸­çš„å…ƒç´ 
    
    å‚æ•°:
        set_a: ç¬¬ä¸€ä¸ªé›†åˆ(æ•°æ®è¾ƒå°‘)
        set_b: ç¬¬äºŒä¸ªé›†åˆ(æ•°æ®è¾ƒå¤š)
        
    è¿”å›:
        å¦‚æœAä¸­æœ‰å…ƒç´ ä¸åœ¨Bä¸­ï¼Œè¿”å›è¿™äº›å…ƒç´ çš„é›†åˆ
        å¦‚æœAä¸­æ‰€æœ‰å…ƒç´ éƒ½åœ¨Bä¸­ï¼Œè¿”å›False
    """
    difference = set_a - set_b  # è®¡ç®—å·®é›†
    if difference:
        return difference
    else :
        return False
    
def find_url_add_car_ele(page,url):
    """
    åŠŸèƒ½ï¼š
    ä»é“¾æ¥ä¸­è·å–æ·»åŠ è´­ç‰©è½¦ä¿¡æ¯
    
    å‚æ•°ï¼š
        url: é“¾æ¥
        
    è¿”å›ï¼š
        è¿”å›è´­ç‰©è½¦çš„å®šä½å…ƒç´ ä¿¡æ¯
    """
    url_ele = page.ele(f"@href={url}").parent(2)
    return url_ele


def car_ele_to_add_car_info(base_ele)->dict:
    """
    åŠŸèƒ½ï¼š
    ä»å®šä½å…ƒç´ ä¸­è·å–æ·»åŠ è´­ç‰©è½¦ä¿¡æ¯
    
    å‚æ•°ï¼š
        url_ele: å®šä½å…ƒç´ 
        
    è¿”å›ï¼š
        è¿”å›è´­ç‰©è½¦çš„ä¿¡æ¯,æ‰“åŒ…æˆå­—å…¸åè¿›è¡Œè¿”å›
    """
    info_str = base_ele.ele("@class=obsess-box product-card__obsess js-obsess-box").attr("data-analytics-attributes")
    info_dict = json.loads(info_str)

    good_id  = str(info_dict["product_id"])
    queryID = info_dict["queryID"]

    _analytics_session_id_ele = base_ele.ele("@@class=obsession-container js-obsess-box-container").ele("t:button")
    session_id = _analytics_session_id_ele.attr("data-analytics-session-id")

    if  not session_id:
        log_to_text("session_idä¸ºç©º")
    if not good_id:
        log_to_text("good_idä¸ºç©º")
    if not queryID:
        log_to_text("queryIDä¸ºç©º")

    return good_id,queryID,session_id


def find_url_add_car_info(page,url):
    """é€šè¿‡ä¼ å…¥URLï¼Œè·å–å•†å“IDï¼ŒqueryIDï¼ŒsessionID"""
    url_ele = find_url_add_car_ele(page,url)
    good_id,queryID,session_id = car_ele_to_add_car_info(url_ele) # è·å–å•†å“ä¿¡æ¯
    return good_id,queryID,session_id

def get_url_set():
    """è·å–æ‰€æœ‰URLçš„é›†åˆ"""
    url_set = read_urls_from_csv()
    gv.set_global_var('global_url_set',url_set)
    # log_to_text("å…¨å±€å˜é‡ä¸­å­˜å‚¨çš„URLé›†åˆï¼š",gv.get_global_var('global_url_set'))


def start_listen_shop():
    """å¼€å§‹ç›‘å¬å•†å“"""
    log_to_text("å¼€å§‹ç›‘å¬å•†å“")

    # TODO åæœŸè¿™é‡Œå¼€å§‹å¯ç”¨å¤šè¿›ç¨‹æ–¹æ¡ˆè¿›è¡Œå•†å“ç›‘å¬
    get_url_set()
    co1 = ChromiumOptions().set_local_port(9226).set_user_data_path('data1')
    page = Chromium(co1)
    url_list = gv.get_global_var('shop_class')
    class_url_base = gv.get_global_var('class_base_url')
    url_list = [class_url_base + url for url in url_list]

    open_urls_concurrently(page,url_list)




if __name__ == '__main__':

    start_listen_shop()
    # test_account()
    # co1 = ChromiumOptions().set_local_port(9226).set_user_data_path('data1')
    # page = Chromium(co1)




    # url = "/products/men/bags/weekenders/chrome-hearts-leather-big-fleur-q38zg"


    # open_url = "https://www.therealreal.com"+url
    # open_url_add_car(page,open_url)

    
  
    # # page.get("https://www.therealreal.com/products?keywords=chrome%20hearts%20bags")













    # url = "https://www.therealreal.com/products?keywords=chrome%20hearts%20bags"
    # headers = {
    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'}
    # url_data = requests.get(url,headers=headers)
    # print(url_data.text)